{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we have a quick look at the Scipy/sklearn's Perceptron Implementation. A lot of their classifiers - through code reuse - have a real similar interface. Usually you'll go something like:\n",
    "```\n",
    "c = Classifier(some_attr=...)\n",
    "c.fit(data,labels)\n",
    "c.score(test_data,test_labels)\n",
    "```\n",
    "\n",
    "For more see `help(Perceptron)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logical_operator_dataset(size, operator):\n",
    "    \"\"\"Generate a random dataset and corresponding target labels of the supplied size and numpy logical operator\"\"\"\n",
    "    \n",
    "    # random ints of zero and one,\n",
    "    x = np.random.randint(2,size=(size, 1))\n",
    "    y = np.random.randint(2,size=(size, 1))\n",
    "    \n",
    "    # Logical operator it, transform True|False values into 1|0 then flatten the column vector to a 1d array\n",
    "    labels = np.ravel(np.where(operator(x,y),1,0)) \n",
    "    data = np.concatenate((x,y),axis=1)\n",
    "    return data,labels\n",
    "\n",
    "data_AND, labels_AND = logical_operator_dataset(500,np.logical_and)\n",
    "data_XOR, labels_XOR = logical_operator_dataset(500,np.logical_xor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logical AND \n",
    "------\n",
    "\n",
    "So we have our logical and dataset `data_AND` with corresponding target labels `labels_AND`, we'd except the perceptron to be easily able to capture the AND relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Perceptron(n_iter= 500)\n",
    "\n",
    "# 400 items in the training set, and 100 in the test set. \n",
    "p.fit(data_AND[:400], labels_AND[:400])\n",
    "p.score(data_AND[400:], labels_AND[400:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, 100%, lets see how it fares on XOR, should be awful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54000000000000004"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Perceptron(n_iter= 500)\n",
    "\n",
    "p.fit(data_XOR[:400], labels_XOR[:400])\n",
    "p.score(data_XOR[400:], labels_XOR[400:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "54%, yup that's awful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
