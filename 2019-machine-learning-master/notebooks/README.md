
# :notebook_with_decorative_cover: notebooks :notebook_with_decorative_cover:

Intro to using 'numpy'
 * [numpy basics](numpy-basics.ipynb)

Some really _really_ simple perceptrons
 * [super simple perceptron](super-simple-Perceptron.ipynb)
 * [moronically simple perceptron](moronic-simple-Perceptron.ipynb)
 * [sklearn's perceptron](sklearn-perceptron.ipynb)
 * there is also Perceptron code from Marsland [here](https://seat.massey.ac.nz/personal/s.r.marsland/MLBook.html)
 
Autograd
 * [example of using autograd](autograd_example.ipynb)
 
PCA
 * [example of PCA](Simple_PCA_example.ipynb)
 
Gibbs Sampling
 * [really simple Gibbs sampling](Gibbs_Simplest_Demo_naive.ipynb) - a bit naive as it's not in logspace
 * [better Gibbs sampling](Gibbs_Simplest_Demo_logspace.ipynb) - smarter as does computations in logspace
 
Bayes
 * [Bayesian coin tossing!](bayes_coin.ipynb)
 * [Bayesian linear regression](BayesianLinearRegression.ipynb)
 * [Bayesian neural net by MCMC](bayes_nn.ipynb)
 
Mixtures
 * [make data from a Mixture of Gaussians distribution](make-Gaussian-mixture-data.ipynb) 
 * [Mixture of Gaussians](MoG_EM_marcus.ipynb)  - not sure which of these two is the "right" one?
 * [Mixture of Gaussians](MoG_EM_mf.ipynb)  - not sure which of these two is the "right" one?
