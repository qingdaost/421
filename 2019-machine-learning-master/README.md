# 2019 Machine Learning

The 2019 incarnation of the VUW Machine Learning course.

# :sunny: Outline :sunny:


Outline of topics / lecture schedule 
(NB. details may change as the course proceeds)

Topic | When (approx) | text
------------ | ------------- | ------------- 
[preliminaries](preliminaries) | week 1 | *Mars 2* and *Mars 3* | [ex 1](exercises/ex1.md)
[backpropagation](backprop) | week 2 | *Mars 4* | [ex 2](exercises/ex2.md)
[optimisation regularisation](optimiseRegularise) | week 3 | mostly from Goodfellow *et al.* | [ex 3](exercises/ex3.md)
[dimensionality reduction](dimreduction) | week 4 | *Mars 6* supplemented by Goodfellow *et al.* 
[deep generative models](deepgen) | week 5 | VAE, GAN...  see Goodfellow *et al.*
[neural nets for sequences](sequencesNN) | week 6 | LSTMs. See Goodfellow *et al.*
**midterm break**
[learning with ensembles](ensembles) | week 7 | *Mars 13* 
[EM for MoG](EMalg) | week 8 | *Mars 7*
[Graphical models and HMM](pgm) | week 9 | *Mars 16*
[Temporal processes](sequencesPGM) | week 10 | *Mars 16*
[kernel methods](kernels) | week 11 | *Mars 16*
[Gaussian Processes](gaussianprocess) | week 12 | *Mars 18* 


# Assessment:

The assessment will consist of:
* 10 small exercises, each worth 3%.
* 2 assignments, each worth 15%. These will be a mixture of theory and practical work.
* An exam, worth 40% of the marks. 

What | contribution | when
------------ | ------------- | -------------
[small exercises](exercises) | 30 % in all | most weeks 
[assignment 1](assign1) | 15 % | out on DATE_TBC, due on DATE_TBC
[assignment 2](assign2) | 15 % | out on DATE_TBC, due on DATE_TBC
[exam](exam) | 40 % | in the exam period


# Textbook
We will make use of 3 texts, all of which can be read online, or purchased hardcopy:
1. Machine Learning, An Algorithmic Perspective (Stephen Marsland).  Available [online](https://tewaharoa.victoria.ac.nz/primo-explore/fulldisplay?docid=TN_pq_ebook_centralEBC1591570&context=PC&vid=VUWNUI&lang=en_NZ&search_scope=64VUW_ALL&adaptor=primo_central_multiple_fe&tab=all&query=any,contains,Marsland%20Machine%20Learning&facet=rtype,exclude,newspaper_articles&facet=rtype,exclude,reviews), under "View It - ProQuest...)"
2. [Deep Learning](http://www.deeplearningbook.org/), by Goodfellow, Bengio and Courville. (free to read online)
3. [Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.HomePage), by David Barber (free to read online). We will only make limited, and very specific, use of this book.

# Class Rep
TODO put here.



# Misc resources pile
* [Colah's blog](http://colah.github.io/)
* [Karpathy's blog](http://karpathy.github.io/)
* [Tensorflow playground](http://playground.tensorflow.org/)
* [Gym for RL](https://gym.openai.com/) - tragically there's no room for Reinforcement Learning in this course however
* Awesome intros to [calc](https://www.youtube.com/watch?v=WUvTyaaNkzM&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr) and [linalg](https://www.youtube.com/watch?v=kjBOesZCoqc&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)
* [Frean's 2017 course](https://gitlab.ecs.vuw.ac.nz/marcus/MachineLearningCOMP421)
* [GAN hacks](https://github.com/soumith/ganhacks)
* [Jupyter docs](http://jupyter.readthedocs.io/en/latest/index.html)
* [Nbtutor: Code Visualization For Jupyter](https://github.com/lgpage/nbtutor)
* [Git cheat sheet](https://services.github.com/on-demand/downloads/github-git-cheat-sheet.pdf)
* [Markdown syntax](https://guides.github.com/pdfs/markdown-cheatsheet-online.pdf)
* [Udacity course in deep learning](https://www.udacity.com/course/deep-learning--ud730)
* [Datacamp course in deep learning](https://www.datacamp.com/courses/deep-learning-in-python)

# Some acronyms:
* PCA = Principal Components Analysis (t.b.d. later, or if you can't wait, see [simple PCA example notebook](https://gitlab.ecs.vuw.ac.nz/marcus/MachineLearningCOMP421/blob/master/notebooks/Simple_PCA_example.ipynb) showing some basic PCA in action)
* SGD = stochastic gradient descent (only stochastic by virtue of randomized minibatches)
* SVM = support vector machine
* GAN = Generative adversarial network (see [this](https://deephunt.in/the-gan-zoo-79597dc8c347) for a :joy:)
* PGM = Probabilistic Graphical Model (including Belief nets, Random Markov Fields, HMMs)
* HMM = Hidden Markov Model


# Want more
If you want to take part in the Machine Learning and Complex Systems research group at ECS, you're very welcome to come along to weekly meetups at the [Festival of Doubt](https://ecs.victoria.ac.nz/Groups/AI/FestivalOfDoubt)





